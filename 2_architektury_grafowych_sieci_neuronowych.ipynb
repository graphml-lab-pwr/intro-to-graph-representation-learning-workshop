{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdac827b",
   "metadata": {},
   "source": [
    "# \"Introduction to Graph Representation Learning\"\n",
    "## Szkoła letnia AI-Tech 2023\n",
    "### Autor: Piotr Bielak\n",
    "\n",
    "![Logotypy sponsorów](sponsors.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45300df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r https://raw.githubusercontent.com/graphml-lab-pwr/intro-to-graph-representation-learning-workshop/master/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796fbad2",
   "metadata": {},
   "source": [
    "## 4. Uczenie reprezentacji grafów\n",
    "\n",
    "Termin określa zbiór metod, w których dla wybranych encji grafowych (wierzchołki, krawędzie, podgrafy, całe grafy) celem jest znalezienie pewnej reprezentacji wektorowej, która odwzorowuje pewne cechy tych encji. Najczęściej rozważanym scenariuszem jest uczenie reprezentacji wierzchołków (**node representation learning**), gdzie celem jest znalezienie funkcji reprezentacji (osadzenia, embeddingu):\n",
    "\n",
    "$$\\Large f_\\theta: \\mathcal{V} \\to \\mathbb{R}^{|\\mathcal{V}| \\times d},$$\n",
    "\n",
    "gdzie $\\theta$ to wyuczalne parametry tej funkcji, a $d$ to wymiarowość reprezentacji. Funkcja $f_\\theta$ przypisuje każdemu wierzchołkowi $d$-wymiarowy wektor liczb rzeczywistych."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49379670",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/1xKxHu6.png)\n",
    "[Kurs CS224W: Machine Learning on Graphs](http://web.stanford.edu/class/cs224w/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251ceb92",
   "metadata": {},
   "source": [
    "## 5. Grafowe sieci neuronowe\n",
    "\n",
    "**Grafowe sieci neuronowe** (ang. *Graph Neural Network*, **GNN**) są przykładem architektury głębokiego uczenia maszynowego, która modeluje funkcję reprezentacji wierzchołków. Większość architektur GNN jest oparta o paradygmat **przekazywania wiadomości**. Wierzchołki wymieniają między sobą wiadomości (wektory cech), aby obliczyć/wyznaczyć wektory reprezentacji."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63ca573",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/v2/resize:fit:1136/1*e8xtqXuqNCBWhzdbF7krtA.png)\n",
    "\n",
    "[Kurs CS224W: Machine Learning on Graphs](http://web.stanford.edu/class/cs224w/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c44b723",
   "metadata": {},
   "source": [
    "![](https://publish-01.obsidian.md/access/1b0b209d26800640324dbdc7d5b5e5b5/Computing/Intelligence/Machine%20Learning/Geometric%20deep%20learning/Screenshot%202022-03-15%20at%2007.40.36.png)\n",
    "[M. M. Bronstein, J. Bruna, T. Cohen, P. Velickovic, Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges (2021)](https://arxiv.org/abs/2104.13478)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b09b0c",
   "metadata": {},
   "source": [
    "W niniejszym zeszycie opiszemy 4 najpopularniejsze architektury grafowych sieci neuronowych, mianowicie:\n",
    "- Graph Convolutional Network (**GCN**)\n",
    "- Graph Sample and Aggregate (**GraphSAGE**)\n",
    "- Graph Attention Network (**GAT**)\n",
    "- Graph Isomorphism Network (**GIN**)\n",
    "\n",
    "Implementacje tych modeli są dostępne w bibliotece PyTorch-Geometric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a798da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "\n",
    "\n",
    "dataset = Planetoid(root=\"/tmp/Cora/\", name=\"Cora\")\n",
    "\n",
    "data = dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7d1b01",
   "metadata": {},
   "source": [
    "### Node attributes visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ffb724",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from sklearn.decomposition import PCA\n",
    "from umap import UMAP\n",
    "\n",
    "\n",
    "def visualize_embeddings(z: torch.Tensor, y: torch.Tensor):\n",
    "    fig, axs = plt.subplots(ncols=2, figsize=(15, 5))\n",
    "\n",
    "    z2d_pca = PCA(n_components=2).fit_transform(z)\n",
    "    z2d_umap = UMAP(n_components=2).fit_transform(z)\n",
    "\n",
    "    sns.scatterplot(\n",
    "        x=z2d_pca[:, 0],\n",
    "        y=z2d_pca[:, 1],\n",
    "        hue=y,\n",
    "        palette=\"Set2\",\n",
    "        ax=axs[0],\n",
    "    )\n",
    "    axs[0].set(title=\"PCA\")\n",
    "\n",
    "    sns.scatterplot(\n",
    "        x=z2d_umap[:, 0],\n",
    "        y=z2d_umap[:, 1],\n",
    "        hue=y,\n",
    "        palette=\"Set2\",\n",
    "        ax=axs[1],\n",
    "    )\n",
    "    axs[1].set(title=\"UMAP\")\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57de0ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = visualize_embeddings(z=data.x, y=data.y)\n",
    "fig.suptitle(\"Node attributes\")\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab47e13",
   "metadata": {},
   "source": [
    "# 5.1. Graph Convolutional Network (GCN)\n",
    "Grafowe sieci neuronowe po raz pierwszy w literaturze były proponowane już w okolicach 2008 roku, jednak dopiero rozwój i popularyzacja uczenia głębokiego pozwoliła na efektywne implementacje. Najpopularniejszym obecnie modelem grafowej sieci neuronowej jest **grafowa konwolucja** (GCN - *Graph Convolutional Network*), która została zaproponowana przez Kipfa w 2016 roku – [artykuł](https://arxiv.org/pdf/1609.02907.pdf). Praca ma już ponad 22 tysiące cytowań i wiele obecnych GNNów jest oparta na niej.\n",
    "\n",
    "Model GCN w każdej warstwie oblicza nowe cechy wierzchołków $H^{(l+1)}$ na podstawie obecnych cech $H^{(l)}$ w następujący sposób:\n",
    "\n",
    "$$H^{(l+1)} = \\hat{D}^{-\\frac{1}{2}}\\hat{A}\\hat{D}^{-\\frac{1}{2}}H^{(l)}W^{(l)},$$\n",
    "gdzie:\n",
    "- $\\hat{A} = A + I$ to macierz sąsiedztwa grafu z dołączonymi pętlami zwrotnymi na każdym wierzchołku (krawędź z danego wierzchołka do samego siebie)\n",
    "- $\\hat{D}$ to macierz stopnii węzłów (macierz diagonalna)\n",
    "- $\\hat{D}^{-\\frac{1}{2}}\\hat{A}\\hat{D}^{-\\frac{1}{2}}$ to tzw. symetryczna normalizacja macierzy sąsiedztwa\n",
    "- $W^{(l)}$ to macierz wyuczalnych parametrów\n",
    "\n",
    "Poprzez dodanie pętli na każdym wierzchołku, uśredniane są cechy zarówno sąsiadów jak i cechy danego wierzchołka. Natomiast symetryczna normalizacja pozwala uwzględnić stopień danego wierzchołka oraz stopień sąsiada.\n",
    "\n",
    "Często definicja powyższej reguły *propagacji* uwzględnia również funkcję aktywacji. W celu uniknięcia pomyłek, tutaj ją pomijamy – PyTorch-Geometric też nie stosuje funkcji aktywacji w implementacjach poszczególnych warstw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a2d3b1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "\n",
    "class GCNModel(nn.Module):\n",
    "    def __init__(self, in_dim: int, hidden_dim: int, out_dim: int):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_dim, hidden_dim)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.conv2 = GCNConv(hidden_dim, out_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        z = self.act1(self.conv1(x, edge_index))\n",
    "        z = self.conv2(z, edge_index)\n",
    "        return z\n",
    "\n",
    "\n",
    "gcn = GCNModel(\n",
    "    in_dim=dataset.num_node_features,\n",
    "    hidden_dim=256,\n",
    "    out_dim=128,\n",
    ")\n",
    "    \n",
    "with torch.no_grad():\n",
    "    z_gcn = gcn(x=data.x, edge_index=data.edge_index)\n",
    "\n",
    "print(z_gcn)\n",
    "\n",
    "print(z_gcn.shape)\n",
    "    \n",
    "\n",
    "fig = visualize_embeddings(z=z_gcn, y=data.y)\n",
    "fig.suptitle(\"GCN - untrained\")\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac6bf0e",
   "metadata": {},
   "source": [
    "**Uwaga:** Dla wielu architektur GNN, Pytorch-Geometric posiada gotowe klasy pozwalające budować wielowarstwowe modele, tj. nie trzeba ręcznie definiować kolejnych warstw i aktywacji (tak jak to powyżej zostało przedstawione). Od teraz, o ile to możliwe, będziemy korzystać z tych gotowych klas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525fa229",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCN\n",
    "\n",
    "gcn_2 = GCN(\n",
    "    in_channels=dataset.num_node_features,\n",
    "    hidden_channels=256,\n",
    "    out_channels=128,\n",
    "    num_layers=2,\n",
    "    act=\"relu\",\n",
    ")\n",
    "    \n",
    "with torch.no_grad():\n",
    "    z_gcn_2 = gcn_2(x=data.x, edge_index=data.edge_index)\n",
    "\n",
    "print(z_gcn_2)\n",
    "\n",
    "print(z_gcn_2.shape)\n",
    "    \n",
    "\n",
    "fig = visualize_embeddings(z=z_gcn_2, y=data.y)\n",
    "fig.suptitle(\"GCN (using wrapper class) - untrained\")\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d340ca4e",
   "metadata": {},
   "source": [
    "## 5.2. Graph Sample and Aggregate (GraphSAGE)\n",
    "W 2017 roku Hamilton opublikował [pracę](https://arxiv.org/pdf/1706.02216.pdf), w której rozważał induktywne uczenie grafowych sieci neuronowych oraz zaproponował sposób na osiągnięcie lepszej skalowalności metod GNNowych. Zaproponowana metoda (a właściwie rodzina metod) opiera się na idei próbkowania sąsiedztwa grafu (losowo wybrany podzbiór sąsiadów danego węzła) i następnie agregacji cech tak uzyskanej próbki sąsiadów. Zostały rozważone 3 metody agregacji: uśrednienie, LSTM oraz max pooling. Metoda była inspirowana bezpośrednio algorytmem Weisfeiler-Lehman test, a reguła propagacji jest zdefiniowana następująco (z perspektywy pojedynczego wierzchołka):\n",
    "\n",
    "$$h^{(l)}_{\\mathcal{N}(u)} = \\text{AGGREGATE}^{(l)}(\\{h^{(l)}_v, \\forall v\\in\\mathcal{N}(u) \\}) $$\n",
    "$$h^{(l+1)}_u = W^{(l)} \\cdot \\text{CONCAT}(h^{(l)}_u, h^{(l)}_{\\mathcal{N}(u)})$$\n",
    "\n",
    "Widzimy zatem, że najpierw agregujemy cechy sąsiadów za pomocą wybranej metody agregacji, a następnie konkatenujemy wektor cech danego wierzchołka ze zagregowanym wektorem sąsiadów, po czym przemnażamy taki wektor przez macierz wag.\n",
    "\n",
    "W PyTorch-Geometricu, metoda GraphSAGE (tutaj: `SAGEConv`) jest zaimplementowana delikatnie inaczej – reguła propagacji jest określona następująco (dla wariantu z uśrednieniem):\n",
    "\n",
    "$$h^{(l+1)}_u = W^{(l)}_1 \\cdot h^{(l)}_u + W^{(l)}_2 \\cdot \\text{mean}_{v\\in\\mathcal{N}(u)} h^{(l)}_v$$\n",
    "\n",
    "Cechy danego wierzchołka i zagregowanego sąsiedztwa są przekształcane przez osobne wyuczalne macierze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e0618e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GraphSAGE\n",
    "\n",
    "\n",
    "graphsage = GraphSAGE(\n",
    "    in_channels=dataset.num_node_features,\n",
    "    hidden_channels=256,\n",
    "    out_channels=128,\n",
    "    num_layers=2,\n",
    "    act=\"relu\",\n",
    ")\n",
    "    \n",
    "with torch.no_grad():\n",
    "    z_graphsage = graphsage(x=data.x, edge_index=data.edge_index)\n",
    "\n",
    "print(z_graphsage)\n",
    "\n",
    "print(z_graphsage.shape)\n",
    "    \n",
    "\n",
    "fig = visualize_embeddings(z=z_graphsage, y=data.y)\n",
    "fig.suptitle(\"GraphSAGE - untrained\")\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0d85b9",
   "metadata": {},
   "source": [
    "## 5.3. Graph Attention Network (GAT)\n",
    "Również w 2017 roku, Velickovic opublikował [pracę](https://arxiv.org/pdf/1710.10903.pdf), która przetłumaczyła mechanizm uwagi znany z przetwarzania języka naturalnego do dziedziny grafów. Powstała metoda nazywana Graph Attention (GAT). Reguła propagacji jest określona następująco:\n",
    "\n",
    "$$h^{(l+1)}_u = \\alpha_{u,u} W^{(l)}h^{(l)}_u + \\sum_{v \\in \\mathcal{N}(u)} \\alpha_{u,v}W^{(l)}h^{(l)}_v $$\n",
    "\n",
    "$$\\alpha_{i, j} = \\frac{\\exp(\\text{LeakyReLU}(a^T[W^{(l)}h^{(l)}_i || W^{(l)}h^{(l)}_j]))}{\\sum_{k\\in i\\cup \\mathcal{N}(i)} \\exp(\\text{LeakyReLU}(a^T[W^{(l)}h^{(l)}_i || W^{(l)}h^{(l)}_k]))},$$\n",
    "\n",
    "gdzie:\n",
    "- $W^{(l)}$ to wyuczalna macierz parametrów\n",
    "- $a$ to wyuczalne parametry mechanizmu uwagi (ang. *attention parameters*)\n",
    "- $\\alpha$ to współczynniki mechanizmu uwagi (ang. *attention coefficients*)\n",
    "\n",
    "Metoda pozwala również na wykorzystanie wielu głowic uwagi (ang. *multi-headed attention*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f60b4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GAT\n",
    "\n",
    "gat = GAT(\n",
    "    in_channels=data.num_node_features,\n",
    "    hidden_channels=256,\n",
    "    out_channels=128,\n",
    "    num_layers=2,\n",
    "    act=\"relu\",\n",
    "    heads=2,\n",
    "    concat=False,\n",
    ")\n",
    "    \n",
    "with torch.no_grad():\n",
    "    z_gat = gat(x=data.x, edge_index=data.edge_index)\n",
    "\n",
    "print(z_gat)\n",
    "\n",
    "print(z_gat.shape)\n",
    "    \n",
    "\n",
    "fig = visualize_embeddings(z=z_gat, y=data.y)\n",
    "fig.suptitle(\"GAT - untrained\")\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dc0dd2",
   "metadata": {},
   "source": [
    "## 5.4. Graph Isomorphism Network (GIN)\n",
    "\n",
    "W [pracy](https://arxiv.org/abs/1810.00826) z 2018, autorzy rozważali temat ekspresywności GNNów (tj. jakie struktury w grafie są GNNy w stanie rozróżniać). Okazało się, że powyżej przedstawione architektury (GCN, GraphSAGE) nie potrafią rozróżniać prostych struktur. Celem pracy było znalezienie najbardziej ekspresywnej architektury w klasie grafowych sieci opartych o przekazywanie wiadomości. W ten sposób, inspirowany testem izomorficzności Weisfeiler-Lehman, powstała architektura GIN:\n",
    "\n",
    "$$h_u^{(l+1)} = \\Theta\\left((1 + \\epsilon) h_u^{(l)} + \\sum_{v \\in \\mathcal{N}(u)} h_v^{(l)}\\right), $$\n",
    "\n",
    "gdzie:\n",
    "- $\\Theta(\\cdot)$ jest sieci neuronową (np. MLP),\n",
    "- $\\epsilon$ jest wyuczalnym parametrem, określającym ważność danego wierzchołka względem jego sąsiadów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555c0309",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch_geometric.nn import GIN\n",
    "\n",
    "\n",
    "gin = GIN(\n",
    "    in_channels=dataset.num_node_features,\n",
    "    hidden_channels=256,\n",
    "    out_channels=128,\n",
    "    num_layers=2,\n",
    "    act=\"relu\",\n",
    "    train_eps=True,\n",
    ")\n",
    "    \n",
    "with torch.no_grad():\n",
    "    z_gin = gin(x=data.x, edge_index=data.edge_index)\n",
    "\n",
    "print(z_gin)\n",
    "\n",
    "print(z_gin.shape)\n",
    "    \n",
    "\n",
    "fig = visualize_embeddings(z=z_gin, y=data.y)\n",
    "fig.suptitle(\"GIN - untrained\")\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad4c3c5",
   "metadata": {},
   "source": [
    "# Zadanie (10 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd68d163",
   "metadata": {},
   "source": [
    "## Z.1. Implementacja własnej warstwy GNN\n",
    "\n",
    "Celu implementacji własnej warstwy grafowej sieci neuronowej należy utworzyć klasę, która dziedziczy po klasie **MessagePassing**. Realizuje ona mechanizm przesyłania wiadomości między wierzchołkami, a użytkownik musi jedynie zdefiniować następujące funkcje:\n",
    "- `message(...)` - jak generowana jest wiadomość?\n",
    "- `update(...)` - jak wierzchołek aktualizuje swoją reprezentację?\n",
    "- `aggr=...` - jak agregowane są wiadomości?, np. `aggr=\"add\"`, `aggr=\"mean\"`\n",
    "\n",
    "Najczęściej funkcja `update(...)` nie musi być implementowana przez użytkownika i wystarczy użyć implementacji z klasy bazowej `MessagePassing`. Zobaczmy najpierw jak zaimplementować warstwę konwolucji grafowej GCN \"od zera\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3c282e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "\n",
    "\n",
    "class CustomGCNConv(MessagePassing):\n",
    "    \"\"\"Based on: `https://pytorch-geometric.readthedocs.io/en/latest/tutorial/create_gnn.html`\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__(aggr=\"add\", flow=\"source_to_target\")   # Aggregate using sum\n",
    "        self.W = nn.Linear(in_channels, out_channels, bias=False)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.W.reset_parameters()\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # x: [|V|, in_channels], edge_index: [2, |E|]\n",
    "\n",
    "        # Add self-loops to the adjacency matrix.\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
    "\n",
    "        # Adjacency matrix normalization (symmetric normalization)\n",
    "        row, col = edge_index\n",
    "        deg = degree(col, x.size(0), dtype=x.dtype)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "        \n",
    "        # Transform features using weight matrix\n",
    "        x = self.W(x)\n",
    "\n",
    "        # Message passing\n",
    "        out = self.propagate(edge_index, x=x, norm=norm)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def message(self, x_j, norm):\n",
    "        # x_j: [|E|, out_channels]\n",
    "\n",
    "        # Apply normalization\n",
    "        return norm.view(-1, 1) * x_j\n",
    "    \n",
    "    \n",
    "    \n",
    "custom_gcn = CustomGCNConv(\n",
    "    in_channels=dataset.num_node_features,\n",
    "    out_channels=128,\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    z_custom_gcn = custom_gcn(x=data.x, edge_index=data.edge_index)\n",
    "    \n",
    "    \n",
    "print(z_custom_gcn)\n",
    "\n",
    "print(z_custom_gcn.shape)\n",
    "    \n",
    "\n",
    "fig = visualize_embeddings(z=z_custom_gcn, y=data.y)\n",
    "fig.suptitle(\"Custom GCN (single layer) - untrained\")\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2ec80c",
   "metadata": {},
   "source": [
    "Zauważmy, że:\n",
    "- w funkcji `__init__(...)`:\n",
    "    * do klasy bazowej przekazujemy argument: **`aggr=\"add\"`**, czyli będziemy wiadomości od sąsiadów **sumować**\n",
    "    * możemy również (opcjonalnie) określić kierunek przepływu wiadomości za pomocą argumentu **`flow=`**, który przyjmuje wartość: **`source_to_target`**  albo **`target_to_source`**\n",
    "    * deklarujemy wszystkie wyuczalne parametry naszej warstwy GNN (tutaj: `self.W`)\n",
    "- w funkcji `forward(...)`:\n",
    "    * dodajemy pętle zwrotne do każdego wierzchołka: $\\hat{\\mathbf{A}} = \\mathbf{A} + \\mathbf{I}$\n",
    "    * obliczamy współczynniki normalizujące (symetryczna normalizacja)\n",
    "    * przekształcamy atrybuty za pomocą macierzy $\\mathbf{W}$\n",
    "    * przesyłamy wiadomości i zapisujemy wyniki agregacji **`self.propagate(...)`**\n",
    "- w funkcji `message(...)`:\n",
    "    * wyznaczamy przesyłaną wiadomość\n",
    "    * cechy wierzchołka końcowego są oznaczane jako `x_i` (target), a wierzchołka początkowego jako `x_j` (source)\n",
    "    * dla cech każdego wierzchołka początkowego `x_j` i wcześniej policzonych współczynników normalizujących obliczamy wiadomość: `norm * x_j`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55ce31e",
   "metadata": {},
   "source": [
    "**Zadanie:** Zaimplementuj warstwę GNN, która ma następująco określoną regułę propagacji:\n",
    "\n",
    "$$\\Large h_u^{(l+1)} = \\Theta_\\text{self}(h_u^{(l)}) + min_{v \\in \\mathcal{N}(u)} \\Theta_\\text{neighbor}(h_v^{(l)} - h_u^{(l)}),$$\n",
    "\n",
    "gdzie $\\Theta_\\text{self}$ oraz $\\Theta_\\text{neighbor}$ to sieci neuronowe MLP składające się z dwóch warstw liniowych (`nn.Linear`) oraz aktywacji `nn.PReLU()` między nimi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ac39a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wpisz rozwiązanie tutaj!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5fc6f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
