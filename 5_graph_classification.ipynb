{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d39eb517",
   "metadata": {},
   "source": [
    "# \"Introduction to Graph Representation Learning\"\n",
    "## Szkoła letnia AI-Tech 2023\n",
    "### Autor: Piotr Bielak\n",
    "\n",
    "![Logotypy sponsorów](sponsors.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db38df64",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r https://raw.githubusercontent.com/graphml-lab-pwr/intro-to-graph-representation-learning-workshop/master/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f64dc6",
   "metadata": {},
   "source": [
    "## 9. Klasyfikacja grafów\n",
    "\n",
    "Zadanie to jest podobne do klasyfikacji wierzchołków, przy czym tutaj klasa jest przypisana do całego grafu. Przykładem może być klasyfikacja białek (reprezentowanych jako grafy). \n",
    "\n",
    "Zbiór grafów dzielimy na zbiór treningowy, walidacyjny oraz testowy i uczymy odpowiedni klasyfikator. Jedynym zagadnieniem, które musimy rozwiązać jest sposób uzyskania wektora reprezentacji dla całego grafu na podstawie wektorów reprezentacji pojedynczych wierzchołków. Będziemy rozważać proste przekształcenia, które będą agregować wektory wierzchołków w jeden wektor opisujacy cały graf:\n",
    "\n",
    "- uśrednianie (ang. **mean pooling**): $z_\\mathcal{G} = \\frac{1}{|\\mathcal{V}|} \\sum_{u \\in \\mathcal{V}} z_u$\n",
    "- redukcja max (ang. **max pooling**): $z_\\mathcal{G} = \\max_i \\{z_1^{(i)}, z_2^{(i)}, \\ldots, z_{|\\mathcal{V}|}^{(i)} \\} \\;\\forall i = 1 \\ldots d$, gdzie $z_u^{(i)}$ oznacza $i$-ty element wektora $z_u$,\n",
    "- sumowanie: $z_\\mathcal{G} =\\sum_{u \\in \\mathcal{V}} z_u$\n",
    "\n",
    "oraz podejście oparte o architekturę DeepSets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7618c3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff2ca0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c925a5a",
   "metadata": {},
   "source": [
    "Jako zbioru do zadania klasyfikacji grafów użyjemy wprowadzonego wcześniej zbioru **ENZYMES**, który jest zbiorem cząsteczek chemicznych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627f3bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import TUDataset\n",
    "\n",
    "\n",
    "dataset = TUDataset(root=\"/tmp/ENZYMES/\", name=\"ENZYMES\", use_node_attr=True)\n",
    "\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e8020c",
   "metadata": {},
   "source": [
    "Podzielimy zbiór teraz na podzbiór treningowy, walidacyjny i testowy oraz wykorzystamy klasę `DataLoader` do utworzenia mini paczek:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7450d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "indices = torch.arange(len(dataset))\n",
    "y = torch.cat([data.y for data in dataset])\n",
    "\n",
    "\n",
    "train_idx, rest_idx = train_test_split(indices, train_size=0.5, stratify=y)\n",
    "val_idx, test_idx = train_test_split(rest_idx, train_size=0.2, stratify=y[rest_idx])\n",
    "\n",
    "\n",
    "print(f\"Train: {len(train_idx) / len(dataset)}\")\n",
    "print(f\"Val: {len(val_idx) / len(dataset)}\")\n",
    "print(f\"Test: {len(test_idx) / len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff30e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "train_loader = DataLoader(dataset[train_idx], batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(dataset[val_idx], batch_size=batch_size)\n",
    "test_loader = DataLoader(dataset[test_idx], batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3e4d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))\n",
    "\n",
    "\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7457d898",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49aca10a",
   "metadata": {},
   "source": [
    "**Uwaga:** Pole `batch` określa który wierzchołek należy do którego grafu, dzięki czemu jesteśmy w stanie odróżnić kilka grafów zapisanych w jednym obiekcie typu `Data(...)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd046ab6",
   "metadata": {},
   "source": [
    "Wykorzystamy teraz gotowe moduły pozwalające na zamianę reprezentacji wierzchołków na reprezentacje grafów, aby zaimplementować model do klasyfikacji grafów:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60061bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch_geometric.nn import (\n",
    "    global_mean_pool,\n",
    "    global_max_pool,\n",
    "    global_add_pool,\n",
    "    DeepSetsAggregation,\n",
    ")\n",
    "\n",
    "\n",
    "class GraphClassificationModel(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        gnn: nn.Module,\n",
    "        emb_dim: int,\n",
    "        num_classes: int,\n",
    "        graph_pooling: str,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.gnn = gnn\n",
    "        self.clf_head = nn.Linear(emb_dim, num_classes)\n",
    "        \n",
    "        if graph_pooling == \"mean\":\n",
    "            self.pooling = global_mean_pool\n",
    "        elif graph_pooling == \"max\":\n",
    "            self.pooling = global_max_pool\n",
    "        elif graph_pooling == \"sum\":\n",
    "            self.pooling = global_add_pool\n",
    "        elif graph_pooling == \"deepsets\":\n",
    "            self.pooling = DeepSetsAggregation()\n",
    "            \n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        edge_index: torch.Tensor,\n",
    "        batch: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        z = self.gnn(x=x, edge_index=edge_index)\n",
    "        z_graph = self.pooling(z, batch)\n",
    "        out = self.clf_head(z_graph)\n",
    "        return F.log_softmax(out, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d30920",
   "metadata": {},
   "source": [
    "Wykorzystując również standardową pętlę uczenia w PyTorchu oraz funkcję entropii krzyżowej możemy nauczyć model klasyfikacji grafów:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2704b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import f1_score\n",
    "from umap import UMAP\n",
    "from tqdm.auto import trange\n",
    "\n",
    "\n",
    "def train(model: nn.Module, num_epochs: int):\n",
    "    losses = {\"train\": []}\n",
    "    f1s = {\"train\": [], \"val\": [], \"test\": []}\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=1e-3,\n",
    "        weight_decay=5e-4,\n",
    "    )\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    for epoch in trange(num_epochs, desc=\"Epochs\"):\n",
    "        # Train\n",
    "        model.train()\n",
    "        \n",
    "        total_loss = 0\n",
    "        for data_batch in train_loader:\n",
    "            data_batch = data_batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            loss = F.nll_loss(\n",
    "                input=model(\n",
    "                    data_batch.x,\n",
    "                    data_batch.edge_index,\n",
    "                    data_batch.batch,\n",
    "                ),\n",
    "                target=data_batch.y,\n",
    "            )\n",
    "        \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += float(loss) * data_batch.num_graphs\n",
    "            \n",
    "        total_loss /= len(train_loader.dataset)\n",
    "        losses[\"train\"].append(total_loss)\n",
    "        \n",
    "        # Log\n",
    "        model.eval()\n",
    "        \n",
    "        for name, loader in (\n",
    "            (\"train\", train_loader),\n",
    "            (\"val\", val_loader),\n",
    "            (\"test\", test_loader),\n",
    "        ):\n",
    "            y_score, y_true = [], []\n",
    "            \n",
    "            for data_batch in loader:\n",
    "                data_batch = data_batch.to(device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    y_score.append(\n",
    "                        model(\n",
    "                            data_batch.x,\n",
    "                            data_batch.edge_index,\n",
    "                            data_batch.batch,\n",
    "                        )\n",
    "                    )\n",
    "                    y_true.append(data_batch.y)\n",
    "                \n",
    "            y_pred = torch.cat(y_score, dim=0).argmax(dim=-1).cpu()\n",
    "            y_true = torch.cat(y_true, dim=0).cpu()\n",
    "        \n",
    "            f1s[name].append(\n",
    "                f1_score(\n",
    "                    y_pred=y_pred,\n",
    "                    y_true=y_true,\n",
    "                    average=\"macro\",\n",
    "                )\n",
    "            )\n",
    "                \n",
    "    # Visualize metrics\n",
    "    fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(15, 10))\n",
    "    axs = axs.ravel()\n",
    "    axs[0].plot(range(num_epochs), losses[\"train\"], label=\"Train\")\n",
    "    axs[0].set(title=\"Loss\")\n",
    "    axs[0].legend()\n",
    "    \n",
    "    axs[1].plot(range(num_epochs), f1s[\"train\"], label=\"Train\")\n",
    "    axs[1].plot(range(num_epochs), f1s[\"val\"], label=\"Val\")\n",
    "    axs[1].plot(range(num_epochs), f1s[\"test\"], label=\"Test\")\n",
    "    best_val_f1 = max(f1s[\"val\"])\n",
    "    best_idx = f1s[\"val\"].index(best_val_f1)\n",
    "    best_test_f1 = f1s[\"test\"][best_idx]\n",
    "    \n",
    "    axs[1].set(title=f\"F1 (best val: {best_val_f1:.4f}, test: {best_test_f1:.4f})\")\n",
    "    axs[1].legend()\n",
    "    \n",
    "    # Visualize final embeddings\n",
    "    graph_embeddings, labels = [], []\n",
    "    \n",
    "    for data_batch in DataLoader(dataset):\n",
    "        data_batch = data_batch.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            z = model.gnn(data_batch.x, data_batch.edge_index)\n",
    "            graph_embeddings.append(model.pooling(z, data_batch.batch).cpu())\n",
    "            labels.append(data_batch.y.cpu())\n",
    "            \n",
    "    graph_embeddings = torch.cat(graph_embeddings, dim=0)\n",
    "    labels = torch.cat(labels, dim=0)\n",
    "        \n",
    "    z2d_pca = PCA(n_components=2).fit_transform(graph_embeddings)\n",
    "    z2d_umap = UMAP(n_components=2).fit_transform(graph_embeddings)\n",
    "\n",
    "    sns.scatterplot(\n",
    "        x=z2d_pca[:, 0],\n",
    "        y=z2d_pca[:, 1],\n",
    "        hue=labels,\n",
    "        palette=\"Set2\",\n",
    "        ax=axs[2],\n",
    "    )\n",
    "    axs[2].set(title=\"PCA\")\n",
    "\n",
    "    sns.scatterplot(\n",
    "        x=z2d_umap[:, 0],\n",
    "        y=z2d_umap[:, 1],\n",
    "        hue=labels,\n",
    "        palette=\"Set2\",\n",
    "        ax=axs[3],\n",
    "    )\n",
    "    axs[3].set(title=\"UMAP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadb24fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCN\n",
    "\n",
    "\n",
    "train(\n",
    "    model=GraphClassificationModel(\n",
    "        gnn=GCN(\n",
    "            in_channels=dataset.num_node_features,\n",
    "            hidden_channels=256,\n",
    "            out_channels=128,\n",
    "            num_layers=2,\n",
    "            act=\"relu\",\n",
    "        ),\n",
    "        emb_dim=128,\n",
    "        num_classes=dataset.num_classes,\n",
    "        graph_pooling=\"mean\",\n",
    "    ),\n",
    "    num_epochs=200,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1010c073",
   "metadata": {},
   "source": [
    "## Zadanie (10 min)\n",
    "\n",
    "## Z.4. Ewaluacja innych sposobów wyznaczania reprezentacji grafów\n",
    "\n",
    "Korzystając z powyższego kodu dokonaj ewaluacji i porównania jakości działania innych metod agregacji reprezentacji wierzchołków w reprezentacje grafowe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92434151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wpisz rozwiązania tutaj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b3f266",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
